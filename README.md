# DSS_thesis_project_2024
Welcome to this page dedicated to the thesis project of the Data Science and Society (business) program. All the used feature extraction models are loaded, plus the code for the Average Entropy metric, and the code for computing the c-TF-IDF score (BERTopic. Grootendorst, 2022). Moreover, the code for a fine-tuning strategy is also uploaded. 

This fine-tuning part of the project (see Feature_extraction_Model_3_FINETUNING) unfortunately did not make the final thesis, due to issues with saving, and loading the fine-tuned model, in combination with time constraints. The idea was to train RobBERT-2023 on a classification task, training it to classify the (main) subject of the texts, instead of fine-tuning it directly on a clustering objective, or completely pre-training the model from the ground up. 
At the second attempt, a validation-accuracy score of 80%, and a validation-F1-score of 79% was reached (num_epochs=3, lr=5e-5, batch_size=16), a significant improvement compared to the first attemt where a 56% validation-accuracy score, and a validation-F1-score of 55% was reached (num_epochs=5, lr=1e-5, batch_size=32). For fine-tuning/training, the AdamW optimizer was used. The idea of this approach was that if the model would be able to identify the main subject of a court case, it would also be able to create better, more meaningful embeddings, which could then be used for clustering. Unfortunately, this approach was not documented in the final thesis because the fine-tuned model was not saved correctly.

For an example of a full-model configuration setup, see TFIDF_example.
